{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smart RAG\n",
    "- Know what you know and what you don't know.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -qU  markdownify  langchain-upstage rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "# UPSTAGE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "llm = ChatUpstage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "solar_summary = \"\"\"\n",
    "SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling\n",
    "\n",
    "We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, \n",
    "demonstrating superior performance in various natural language processing (NLP) tasks. \n",
    "Inspired by recent efforts to efficiently up-scale LLMs, \n",
    "we present a method for scaling LLMs called depth up-scaling (DUS), \n",
    "which encompasses depthwise scaling and continued pretraining.\n",
    "In contrast to other LLM up-scaling methods that use mixture-of-experts, \n",
    "DUS does not require complex changes to train and inference efficiently. \n",
    "We show experimentally that DUS is simple yet effective \n",
    "in scaling up high-performance LLMs from small ones. \n",
    "Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, \n",
    "a variant fine-tuned for instruction-following capabilities, \n",
    "surpassing Mixtral-8x7B-Instruct. \n",
    "SOLAR 10.7B is publicly available under the Apache 2.0 license, \n",
    "promoting broad access and application in the LLM field.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "from langchain_core.tools import tool\n",
    "import requests\n",
    "import os\n",
    "from tavily import TavilyClient\n",
    "\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "\n",
    "\n",
    "@tool\n",
    "def solar_paper_search(query: str) -> str:\n",
    "    \"\"\"Query for research paper about solarllm, dus, llm and general AI.\n",
    "    If the query is about DUS, Upstage, AI related topics, use this.\n",
    "    \"\"\"\n",
    "    return solar_summary\n",
    "\n",
    "\n",
    "@tool\n",
    "def internet_search(query: str) -> str:\n",
    "    \"\"\"This is for query for internet search engine like Google.\n",
    "    Query for general topics.\n",
    "    \"\"\"\n",
    "    return tavily.search(query=query)\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_news(topic: str) -> str:\n",
    "    \"\"\"Get latest news about a topic.\n",
    "    If users are more like recent news, use this.\n",
    "    \"\"\"\n",
    "    # https://newsapi.org/v2/everything?q=tesla&from=2024-04-01&sortBy=publishedAt&apiKey=API_KEY\n",
    "    # change this to request news from a real API\n",
    "    news_url = f\"https://newsapi.org/v2/everything?q={topic}&apiKey={os.environ['NEWS_API_KEY']}\"\n",
    "    respnse = requests.get(news_url)\n",
    "    return respnse.json()\n",
    "\n",
    "\n",
    "tools = [solar_paper_search, internet_search, get_news]\n",
    "\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_tool(tool_call):\n",
    "    tool_name = tool_call[\"name\"].lower()\n",
    "    if tool_name not in globals():\n",
    "        print(\"Tool not found\", tool_name)\n",
    "        return None\n",
    "    selected_tool = globals()[tool_name]\n",
    "    return selected_tool.invoke(tool_call[\"args\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'solar_paper_search',\n",
       "  'args': {'query': 'Solar LLM'},\n",
       "  'id': 'cb1687d2-7c6a-45dc-8287-19376c335cd4'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"What is Solar LLM?\").tool_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_news',\n",
       "  'args': {'topic': 'Seoul'},\n",
       "  'id': '9f0829a2-da28-4f39-9832-14d07df59eb0'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"What is top news about Seoul\").tool_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'internet_search',\n",
       "  'args': {'query': 'best place in Seoul'},\n",
       "  'id': '1f86d563-de15-460a-abc0-0e644e284518'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"What's best place in Seoul?\").tool_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please provide answer for question from the following context. \n",
    "    ---\n",
    "    Question: {question}\n",
    "    ---\n",
    "    Context: {context}\n",
    "    \"\"\"\n",
    ")\n",
    "chain = prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smart RAG, Self-Improving RAG\n",
    "import os\n",
    "from tavily import TavilyClient\n",
    "\n",
    "\n",
    "def tool_rag(question):\n",
    "    for _ in range(3): # try 3 times\n",
    "        tool_calls = llm_with_tools.invoke(question).tool_calls\n",
    "        if tool_calls:\n",
    "            break\n",
    "\n",
    "    if not tool_calls:\n",
    "        return \"I'm sorry, I don't have an answer for that.\"\n",
    "    \n",
    "    print(tool_calls)\n",
    "    context = \"\"\n",
    "    for tool_call in tool_calls:\n",
    "        context += str(call_tool(tool_call))\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "    return chain.invoke({\"context\": context, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'solar_paper_search', 'args': {'query': 'What is Solar llm?'}, 'id': 'cb291b01-a1aa-4839-84a8-a473f4eb0920'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Solar llm is a large language model (LLM) with 10.7 billion parameters.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_rag(\"What is Solar llm?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'get_news', 'args': {'topic': 'Tesla'}, 'id': 'aade5002-b9e2-4a23-92d7-fd66f12cfeb6'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The news about Tesla is that the company has issued a voluntary recall for nearly 4,000 Cybertrucks due to a fault with the accelerator pedal that could get trapped, pushing the car to full speed. Additionally, Tesla has announced plans to lay off more than 10% of its workforce and is facing a federal investigation into its self-driving claims. The company is also reportedly laying off hundreds more, including the majority of its Supercharging team, and has slashed prices for its vehicles in the US, China, and Europe. Tesla's CEO, Elon Musk, has also been in the news for his recent trip to China to discuss enabling autonomous driving mode on Tesla cars in the country.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_rag(\"What is news about Tesla?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'internet_search', 'args': {'query': 'iPhone 13 spec'}, 'id': '7b89b621-fd4b-4bfe-9eec-dac21354d93c'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The iPhone 13 specs include a 6.1-inch display with a 2532 x 1170 pixel resolution and a 60Hz refresh rate. It is equipped with the Apple A15 Bionic chipset, 4GB of RAM, and 128GB of storage that is not expandable. The device has a 12MP dual camera and a 12MP front camera. The iPhone 13 has a ceramic shield glass and a weight of 6.14 ounces. It has a built-in stereo speaker and microphone, as well as a Lightning connector. The battery life is up to 19 hours for video playback and up to 75 hours for audio playback. The device also supports various languages and has built-in accessibility features.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_rag(\"iPhone 13 spec?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise\n",
    "Solar LLM is small, so it might not give the best results for complex tasks. For those, you can use larger LLMs like GPT-4. However, for quick answers and summaries, using a small size LLM like Solar can give better performance and efficiency.\n",
    "\n",
    "Please note that good engineering involves making things work with limited components.\n",
    "![Engineering](figures/engineering.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
