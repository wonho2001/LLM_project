{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No.1 accuracy in multiform table extraction \n",
    "- Convert documents to maximize RAG performance \n",
    "- LangChain provides powerful tools for text splitting and vectorization\n",
    "\n",
    "\n",
    "![Layout Analyzer](./figures/la.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -qU  markdownify  langchain-upstage  requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "# UPSTAGE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Layout Analyzer](./figures/solar_sample.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<table id='0' style='font-size:14px'><tr><td>Model</td><td>Size</td><td>Type</td><td>H6 (Avg.)</td><td>ARC</td><td>HellaSwag</td><td>MMLU</td><td>TruthfulQA</td><td>Winogrande</td><td>GSM8K</td></tr><tr><td>SOLAR 10.7B-Instruct</td><td>~ 11B</td><td>Alignment-tuned</td><td>74.20</td><td>71.08</td><td>88.16</td><td>66.21</td><td>71.43</td><td>83.58</td><td>64.75</td></tr><tr><td>Qwen 72B</td><td>~ 72B</td><td>Pretrained</td><td>73.60</td><td>65.19</td><td>85.94</td><td>77.37</td><td>60.19</td><td>82.48</td><td>70.43</td></tr><tr><td>Mixtral 8x7B-Instruct-v0.1</td><td>~ 47B</td><td>Instruction-tuned</td><td>72.62</td><td>70.22</td><td>87.63</td><td>71.16</td><td>64.58</td><td>81.37</td><td>60.73</td></tr><tr><td>Yi 34B-200K</td><td>~ 34B</td><td>Pretrained</td><td>70.81</td><td>65.36</td><td>85.58</td><td>76.06</td><td>53.64</td><td>82.56</td><td>61.64</td></tr><tr><td>Yi 34B</td><td>~ 34B</td><td>Pretrained</td><td>69.42</td><td>64.59</td><td>85.69</td><td>76.35</td><td>56.23</td><td>83.03</td><td>50.64</td></tr><tr><td>Mixtral 8x7B-v0.1</td><td>~ 47B</td><td>Pretrained</td><td>68.42</td><td>66.04</td><td>86.49</td><td>71.82</td><td>46.78</td><td>81.93</td><td>57.47</td></tr><tr><td>Llama 2 70B</td><td>~ 70B</td><td>Pretrained</td><td>67.87</td><td>67.32</td><td>87.33</td><td>69.83</td><td>44.92</td><td>83.74</td><td>54.06</td></tr><tr><td>Falcon 180B</td><td>~ 180B</td><td>Pretrained</td><td>67.85</td><td>69.45</td><td>88.86</td><td>70.50</td><td>45.47</td><td>86.90</td><td>45.94</td></tr><tr><td>SOLAR 10.7B</td><td>~ 11B</td><td>Pretrained</td><td>66.04</td><td>61.95</td><td>84.60</td><td>65.48</td><td>45.04</td><td>83.66</td><td>55.50</td></tr><tr><td>Qwen 14B</td><td>~ 14B</td><td>Pretrained</td><td>65.86</td><td>58.28</td><td>83.99</td><td>67.70</td><td>49.43</td><td>76.80</td><td>58.98</td></tr><tr><td>Mistral 7B-Instruct-v0.2</td><td>~ 7B</td><td>Instruction-tuned</td><td>65.71</td><td>63.14</td><td>84.88</td><td>60.78</td><td>68.26</td><td>77.19</td><td>40.03</td></tr><tr><td>Yi 34B-Chat</td><td>~34B</td><td>Instruction-tuned</td><td>65.32</td><td>65.44</td><td>84.16</td><td>74.90</td><td>55.37</td><td>80.11</td><td>31.92</td></tr><tr><td>Mistral 7B</td><td>~ 7B</td><td>Pretrained</td><td>60.97</td><td>59.98</td><td>83.31</td><td>64.16</td><td>42.15</td><td>78.37</td><td>37.83</td></tr></table><p id='1' style='font-size:14px'>Table 2: Evaluation results in the Open LLM Leaderboard for SOLAR 10.7B and SOLAR 10.7B-Instruct along with<br>other top-performing models. We report the scores for the six tasks mentioned in Sec. 4.1 along with the H6 score<br>(average of six tasks). We also report the size of the models in units of billions of parameters. The type indicates the<br>training stage of the model and is chosen from {Pretrained, Instruction-tuned, Alignment-tuned}. Models based on<br>SOLAR 10.7B are colored purple. The best scores for H6 and the individual tasks are shown in bold.</p><p id='2' style='font-size:20px'>MetaMathQA (Yu et al., 2023) dataset.</p><br><p id='3' style='font-size:18px'>We reformatted the instruction datasets with an<br>Alpaca-styled chat template. For datasets such as<br>OpenOrca, which are derived from FLAN (Long-<br>pre et al., 2023), we filter data that overlaps with<br>the benchmark datasets (see Tab. 8 in Appendix. C<br>for more information). The alignment datasets<br>are in the {prompt, chosen, rejected} triplet for-<br>mat. We preprocess the alignment datasets follow-<br>ing Zephyr (Tunstall et al., 2023). We use Data-<br>verse (Park et al., 2024) for data preprocessing.</p><p id='4' style='font-size:18px'>Evaluation. In the HuggingFace Open LLM<br>Leaderboard (Beeching et al., 2023), six types of<br>evaluation methods are presented: ARC (Clark<br>et al., 2018), HellaSWAG (Zellers et al., 2019),<br>MMLU (Hendrycks et al., 2020), TruthfulQA (Lin<br>et al., 2022), Winogrande (Sakaguchi et al., 2021),<br>and GSM8K (Cobbe et al., 2021). We utilize these<br>datasets as benchmarks for evaluation and also re-<br>port the average scores for the six tasks, e.g., H6.<br>We either submit directly to the Open LLM Leader-<br>board or utilize Evalverse (Kim et al., 2024b) for<br>running evaluations locally.</p><p id='5' style='font-size:18px'>Model merging. Model merging methods such<br>as Yadav et al. (2023) can boost model perfor-<br>mance without further training. We merge some<br>of the models that we trained in both the instruc-<br>tion and alignment tuning stages. We implement<br>our own merging methods although popular open<br>source also exist such as MergeKit3.</p><p id='6' style='font-size:18px'>4.2 Main Results</p><br><p id='7' style='font-size:16px'>We present evaluation results for our SOLAR<br>10.7B and SOLAR 10.7B-Instruct models along</p><br><p id='8' style='font-size:18px'>3https: / / github. com/ cg123 /mergekit</p><br><p id='9' style='font-size:18px'>with other top-performing models in Tab. 2. SO-<br>LAR 10.7B outperforms other pretrained models<br>of similar sizes, such as Qwen 14B and Mistral<br>7B, which shows that DUS is an effective method<br>to up-scale base LLMs. Furthermore, despite the<br>smaller size, SOLAR 10.7B-Instruct scores the<br>highest in terms of H6, even surpassing the recent<br>top-performing open-source LLM Mixtral 8x7B-<br>Instruct-v0.1 or Qwen 72B. The above results indi-<br>cate DUS can up-scale models that are capable of<br>achieving state-of-the-art performance when fine-<br>tuned. We also report data contamination results<br>for SOLAR 10.7B-Instruct in Appendix C.</p><p id='10' style='font-size:18px'>4.3 Ablation Studies</p><p id='11' style='font-size:18px'>We present ablation studies for both the instruction<br>and alignment tuning stages. Note that the evalua-<br>tion results for the following studies are ran locally<br>and may vary from results obtained by submitting<br>to the Open LLM Leaderboard.</p><p id='12' style='font-size:22px'>4.3.1 Instruction Tuning</p><br><p id='13' style='font-size:18px'>Ablation on the training datasets. We present<br>ablation studies using different training datasets<br>for the instruction tuning in Tab. 3. The ablated<br>models are prefixed with SFT for supervised fine-<br>tuning. 'SFT v1' only uses the Alpaca-GPT4<br>dataset, whereas 'SFT v2' also uses the OpenOrca<br>dataset. 'SFT v3' uses the Synth. Math-Instruct<br>dataset along with the datasets used in 'SFT v2'.<br>Similarly, 'SFT v4' uses the Synth. Math-Instruct<br>dataset along with the datasets used in 'SFT v1'.</p><br><p id='14' style='font-size:18px'>First, we analyze how Alpaca-GPT4 and<br>OpenOrca affect the trained models. The first ab-<br>lated model, 'SFT v1' , which used only the Alpaca-<br>GPT4 dataset for training, resulted in 69.15 for H6.</p>\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"https://api.upstage.ai/v1/document-ai/layout-analysis\"\n",
    "headers = {\"Authorization\": f\"Bearer {os.getenv('UPSTAGE_API_KEY')}\"}\n",
    "files = {\"document\": open(\"pdfs/solar_sample.pdf\", \"rb\")}\n",
    "response = requests.post(url, headers=headers, files=files)\n",
    "response_json = response.json()\n",
    "response_json[\"html\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/docai.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance - Medical expense statement Customer testimony\n",
      "Percentage \"Achieved a score exceeding 95% on the 5\n",
      "types of documents, exceeding the\n",
      "96\n",
      "previous human works\" - Hanwha Life\n",
      "92~95\n",
      "75~87 \"Tested 7 difficult documents with highly\n",
      "unstructured data, achieving over 95%\n",
      "solid result\" - Samsung Life\n",
      "\"We consistently use the Upstage universal\n",
      "OCR model, which has close to 98%\n",
      "accuracy\" - KB Kookmin Bank\n",
      "\"We are introducing OCR tasks to innovate\n",
      "franchise business opening and ID\n",
      "verification processes\" - Samsung\n",
      "Upstage Human Competitors Securities / Samsung Card\n",
      "Hanwha Life HYUNDAI SERVICE\n",
      "GLOBAL\n",
      "SAMSUNG posco\n",
      "SAMSUNG\n",
      "LIFE INSURANCE\n",
      "Amass\n",
      ". 퍼플스\n",
      "Kb KB Financial Group\n",
      "pay 손해보험\n",
      "Shinhan Bank\n",
      "삼성카드 s AMSUNG\n",
      "SAMSUNG\n",
      "삼성증권\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    url, headers=headers, files={\"document\": open(\"pdfs/docai.pdf\", \"rb\")}\n",
    ")\n",
    "response_json = response.json()\n",
    "for element in response_json[\"elements\"]:\n",
    "    if element[\"category\"] == \"figure\":\n",
    "        print(element[\"text\"])  # or, element[\"html\"]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p id='0' style='font-size:20px'>Document AI: Digitize anything</p><p id='1' style='font-size:18px'>Upstage Document AI is comprised of three solutions:</p><p id='2' style='font-size:16px'>· Layout Analyzer: Extracts layouts, tables, and figures from any document.<br>· Key Information Extractor: Extracts key information from target documents.<br>· Document OCR: Extracts all text from any document.</p><p id='3' style='font-size:18px'>In various enterprise use cases, Upstage Document AI resulted in higher accuracy<br>(95%-98% accuracy rate) compared to previous manual human work for document<br>processing. Due to its strong performance, the solution has been heavily adopted by various<br>leading insurance providers, yielding significant productivity gains and operating cost<br>reductions.</p><p id='4' style='font-size:18px'>Please find customer testimonials, key customers, and customer success stories below.</p><figure><img id='5' style='font-size:14px' alt=\"Performance - Medical expense statement Customer testimony\n",
       "Percentage 'Achieved a score exceeding 95% on the 5\n",
       "types of documents, exceeding the\n",
       "96 · Hanwha Life\n",
       "previous human works'\n",
       "92~95\n",
       "75~87 'Tested 7 difficult documents with highly\n",
       "unstructured data, achieving over 95%\n",
       "solid result' = Samsung Life\n",
       "'We consistently use the Upstage universal\n",
       "OCR model, which has close to 98%\n",
       "accuracy' - KB Kookmin Bank\n",
       "'We are introducing OCR tasks to innovate\n",
       "franchise business opening and ID\n",
       "verification processes' - Samsung\n",
       "Upstage Human Competitors Securities / Samsung Card\n",
       "Hanwha Life HYUNDAI SERVICE\n",
       "GLOBAL\n",
       "SAMSUNG posco\n",
       "SAMSUNG\n",
       "LIFE INSURANCE\n",
       "Amass\n",
       "b KB Financial Group\n",
       "· pay 손해보험\n",
       "Shinhan Bank\n",
       "삼성카드 SAMSUNG\n",
       "SA MSUNG\n",
       "삼성증권\n",
       "[Key customers]\" data-coord=\"top-left:(164,509); bottom-right:(777,1149)\" /></figure>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "files = {\"document\": open(\"figures/docai.png\", \"rb\")}\n",
    "response = requests.post(url, headers=headers, files=files)\n",
    "response_json = response.json()\n",
    "\n",
    "display(HTML(response_json[\"html\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageLayoutAnalysisLoader\n",
    "\n",
    "\n",
    "layzer = UpstageLayoutAnalysisLoader(\"pdfs/solar_sample.pdf\", output_type=\"html\")\n",
    "# For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
    "docs = layzer.load()  # or layzer.lazy_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table id='0' style='font-size:14px'><tr><td>Model</td><td>Size</td><td>Type</td><td>H6 (Avg.)</td><td>ARC</td><td>HellaSwag</td><td>MMLU</td><td>TruthfulQA</td><td>Winogrande</td><td>GSM8K</td></tr><tr><td>SOLAR 10.7B-Instruct</td><td>⇠ 11B</td><td>Alignment-tuned</td><td>74.20</td><td>71.08</td><td>88.16</td><td>66.21</td><td>71.43</td><td>83.58</td><td>64.75</td></tr><tr><td>Qwen 72B</td><td>⇠ 72B</td><td>Pretrained</td><td>73.60</td><td>65.19</td><td>85.94</td><td>77.37</td><td>60.19</td><td>82.48</td><td>70.43</td></tr><tr><td>Mixtral 8x7B-Instruct-v0.1</td><td>⇠ 47B</td><td>Instruction-tuned</td><td>72.62</td><td>70.22</td><td>87.63</td><td>71.16</td><td>64.58</td><td>81.37</td><td>60.73</td></tr><tr><td>Yi 34B-200K</td><td>⇠ 34B</td><td>Pretrained</td><td>70.81</td><td>65.36</td><td>85.58</td><td>76.06</td><td>53.64</td><td>82.56</td><td>61.64</td></tr><tr><td>Yi 34B</td><td>⇠ 34B</td><td>Pretrained</td><td>69.42</td><td>64.59</td><td>85.69</td><td>76.35</td><td>56.23</td><td>83.03</td><td>50.64</td></tr><tr><td>Mixtral 8x7B-v0.1</td><td>⇠ 47B</td><td>Pretrained</td><td>68.42</td><td>66.04</td><td>86.49</td><td>71.82</td><td>46.78</td><td>81.93</td><td>57.47</td></tr><tr><td>Llama 2 70B</td><td>⇠ 70B</td><td>Pretrained</td><td>67.87</td><td>67.32</td><td>87.33</td><td>69.83</td><td>44.92</td><td>83.74</td><td>54.06</td></tr><tr><td>Falcon 180B</td><td>⇠ 180B</td><td>Pretrained</td><td>67.85</td><td>69.45</td><td>88.86</td><td>70.50</td><td>45.47</td><td>86.90</td><td>45.94</td></tr><tr><td>SOLAR 10.7B</td><td>⇠ 11B</td><td>Pretrained</td><td>66.04</td><td>61.95</td><td>84.60</td><td>65.48</td><td>45.04</td><td>83.66</td><td>55.50</td></tr><tr><td>Qwen 14B</td><td>⇠ 14B</td><td>Pretrained</td><td>65.86</td><td>58.28</td><td>83.99</td><td>67.70</td><td>49.43</td><td>76.80</td><td>58.98</td></tr><tr><td>Mistral 7B-Instruct-v0.2</td><td>⇠ 7B</td><td>Instruction-tuned</td><td>65.71</td><td>63.14</td><td>84.88</td><td>60.78</td><td>68.26</td><td>77.19</td><td>40.03</td></tr><tr><td>Yi 34B-Chat</td><td>⇠ 34B</td><td>Instruction-tuned</td><td>65.32</td><td>65.44</td><td>84.16</td><td>74.90</td><td>55.37</td><td>80.11</td><td>31.92</td></tr><tr><td>Mistral 7B</td><td>⇠ 7B</td><td>Pretrained</td><td>60.97</td><td>59.98</td><td>83.31</td><td>64.16</td><td>42.15</td><td>78.37</td><td>37.83</td></tr></table><br><p id='1' style='font-size:16px'>Table 2: Evaluation results in the Open LLM Leaderboard for SOLAR 10.7B and SOLAR 10.7B-Instruct along with<br>other top-performing models. We report the scores for the six tasks mentioned in Sec. 4.1 along with the H6 score<br>(average of six tasks). We also report the size of the models in units of billions of parameters. The type indicates the<br>training stage of the model and is chosen from {Pretrained, Instruction-tuned, Alignment-tuned}. Models based on<br>SOLAR 10.7B are colored purple. The best scores for H6 and the individual tasks are shown in bold.</p><p id='2' style='font-size:20px'>MetaMathQA ( Yu et al. , 2023 ) dataset.</p><br><p id='3' style='font-size:20px'>We reformatted the instruction datasets with an<br>Alpaca-styled chat template. For datasets such as<br>OpenOrca, which are derived from FLAN ( Long-<br>pre et al. , 2023 ), we ﬁlter data that overlaps with<br>the benchmark datasets (see Tab. 8 in Appendix. C<br>for more information). The alignment datasets<br>are in the {prompt, chosen, rejected} triplet for-<br>mat. We preprocess the alignment datasets follow-<br>ing Zephyr ( Tunstall et al. , 2023 ). We use Data-<br>verse ( Park et al. , 2024 ) for data preprocessing.</p><br><p id='4' style='font-size:20px'>Evaluation. In the HuggingFace Open LLM<br>Leaderboard ( Beeching et al. , 2023 ), six types of<br>evaluation methods are presented: ARC ( Clark<br>et al. , 2018 ), HellaSWAG ( Zellers et al. , 2019 ),<br>MMLU ( Hendrycks et al. , 2020 ), TruthfulQA ( Lin<br>et al. , 2022 ), Winogrande ( Sakaguchi et al. , 2021 ),<br>and GSM8K ( Cobbe et al. , 2021 ). We utilize these<br>datasets as benchmarks for evaluation and also re-<br>port the average scores for the six tasks, e.g., H6.<br>We either submit directly to the Open LLM Leader-<br>board or utilize Evalverse ( Kim et al. , 2024b ) for<br>running evaluations locally.</p><br><p id='5' style='font-size:18px'>Model merging. Model merging methods such<br>as Yadav et al. ( 2023 ) can boost model perfor-<br>mance without further training. We merge some<br>of the models that we trained in both the instruc-<br>tion and alignment tuning stages. We implement<br>our own merging methods although popular open<br>source also exist such as MergeKit 3 .</p><br><p id='6' style='font-size:22px'>4.2 Main Results</p><br><p id='7' style='font-size:20px'>We present evaluation results for our SOLAR<br>10.7B and SOLAR 10.7B-Instruct models along</p><br><p id='8' style='font-size:14px'>3 https://github.com/cg123/mergekit</p><br><p id='9' style='font-size:20px'>with other top-performing models in Tab. 2 . SO-<br>LAR 10.7B ou"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(docs[0].page_content[:5000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "| Model | Size | Type | H6 (Avg.) | ARC | HellaSwag | MMLU | TruthfulQA | Winogrande | GSM8K |\n",
       "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
       "| SOLAR 10.7B-Instruct | ⇠ 11B | Alignment-tuned | 74.20 | 71.08 | 88.16 | 66.21 | 71.43 | 83.58 | 64.75 |\n",
       "| Qwen 72B | ⇠ 72B | Pretrained | 73.60 | 65.19 | 85.94 | 77.37 | 60.19 | 82.48 | 70.43 |\n",
       "| Mixtral 8x7B-Instruct-v0.1 | ⇠ 47B | Instruction-tuned | 72.62 | 70.22 | 87.63 | 71.16 | 64.58 | 81.37 | 60.73 |\n",
       "| Yi 34B-200K | ⇠ 34B | Pretrained | 70.81 | 65.36 | 85.58 | 76.06 | 53.64 | 82.56 | 61.64 |\n",
       "| Yi 34B | ⇠ 34B | Pretrained | 69.42 | 64.59 | 85.69 | 76.35 | 56.23 | 83.03 | 50.64 |\n",
       "| Mixtral 8x7B-v0.1 | ⇠ 47B | Pretrained | 68.42 | 66.04 | 86.49 | 71.82 | 46.78 | 81.93 | 57.47 |\n",
       "| Llama 2 70B | ⇠ 70B | Pretrained | 67.87 | 67.32 | 87.33 | 69.83 | 44.92 | 83.74 | 54.06 |\n",
       "| Falcon 180B | ⇠ 180B | Pretrained | 67.85 | 69.45 | 88.86 | 70.50 | 45.47 | 86.90 | 45.94 |\n",
       "| SOLAR 10.7B | ⇠ 11B | Pretrained | 66.04 | 61.95 | 84.60 | 65.48 | 45.04 | 83.66 | 55.50 |\n",
       "| Qwen 14B | ⇠ 14B | Pretrained | 65.86 | 58.28 | 83.99 | 67.70 | 49.43 | 76.80 | 58.98 |\n",
       "| Mistral 7B-Instruct-v0.2 | ⇠ 7B | Instruction-tuned | 65.71 | 63.14 | 84.88 | 60.78 | 68.26 | 77.19 | 40.03 |\n",
       "| Yi 34B-Chat | ⇠ 34B | Instruction-tuned | 65.32 | 65.44 | 84.16 | 74.90 | 55.37 | 80.11 | 31.92 |\n",
       "| Mistral 7B | ⇠ 7B | Pretrained | 60.97 | 59.98 | 83.31 | 64.16 | 42.15 | 78.37 | 37.83 |\n",
       "\n",
       "  \n",
       "Table 2: Evaluation results in the Open LLM Leaderboard for SOLAR 10.7B and SOLAR 10.7B-Instruct along with  \n",
       "other top-performing models. We report the scores for the six tasks mentioned in Sec. 4.1 along with the H6 score  \n",
       "(average of six tasks). We also report the size of the models in units of billions of parameters. The type indicates the  \n",
       "training stage of the model and is chosen from {Pretrained, Instruction-tuned, Alignment-tuned}. Models based on  \n",
       "SOLAR 10.7B are colored purple. The best scores for H6 and the individual tasks are shown in bold.\n",
       "\n",
       "MetaMathQA ( Yu et al. , 2023 ) dataset.\n",
       "\n",
       "  \n",
       "We reformatted the instruction datasets with an  \n",
       "Alpaca-styled chat template. For datasets such as  \n",
       "OpenOrca, which are derived from FLAN ( Long-  \n",
       "pre et al. , 2023 ), we ﬁlter data that overlaps with  \n",
       "the benchmark datasets (see Tab. 8 in Appendix. C  \n",
       "for more information). The alignment datasets  \n",
       "are in the {prompt, chosen, rejected} triplet for-  \n",
       "mat. We preprocess the alignment datasets follow-  \n",
       "ing Zephyr ( Tunstall et al. , 2023 ). We use Data-  \n",
       "verse ( Park et al. , 2024 ) for data preprocessing.\n",
       "\n",
       "  \n",
       "Evaluation. In the HuggingFace Open LLM  \n",
       "Leaderboard ( Beeching et al. , 2023 ), six types of  \n",
       "evaluation methods are presented: ARC ( Clark  \n",
       "et al. , 2018 ), HellaSWAG ( Zellers et al. , 2019 ),  \n",
       "MMLU ( Hendrycks et al. , 2020 ), TruthfulQA ( Lin  \n",
       "et al. , 2022 ), Winogrande ( Sakaguchi et al. , 2021 ),  \n",
       "and GSM8K ( Cobbe et al. , 2021 ). We utilize these  \n",
       "datasets as benchmarks for evaluation and also re-  \n",
       "port the average scores for the six tasks, e.g., H6.  \n",
       "We either submit directly to the Open LLM Leader-  \n",
       "board or utilize Evalverse ( Kim et al. , 2024b ) for  \n",
       "running evaluations locally.\n",
       "\n",
       "  \n",
       "Model merging. Model merging methods such  \n",
       "as Yadav et al. ( 2023 ) can boost model perfor-  \n",
       "mance without further training. We merge some  \n",
       "of the models that we trained in both the instruc-  \n",
       "tion and alignment tuning stages. We implement  \n",
       "our own merging methods although popular open  \n",
       "source also exist such as MergeKit 3 .\n",
       "\n",
       "  \n",
       "4.2 Main Results\n",
       "\n",
       "  \n",
       "We present evaluation results for our SOLAR  \n",
       "10.7B and SOLAR 10.7B-Instruct models along\n",
       "\n",
       "  \n",
       "3 https://github.com/cg123/mergekit\n",
       "\n",
       "  \n",
       "with other top-performing models in Tab. 2 . SO-  \n",
       "LAR 10.7B outperforms other pretrained models  \n",
       "of similar sizes, such as Qwen 14B and Mistral  \n",
       "7B, which shows that DUS is an effective method  \n",
       "to up-scale base LLMs. Furthermore, despite the  \n",
       "smaller size, SOLAR 10.7B-Instruct scores the  \n",
       "highest in terms of H6, even surpassing the recent  \n",
       "top-performing open-source LLM Mixtral 8x7B-  \n",
       "Instruct-v0.1 or Qwen 72B. The above results indi-  \n",
       "cate DUS can up-scale models that are capable of  \n",
       "achieving state-of-the-art performance when ﬁne-  \n",
       "tuned. We also report data contamination results  \n",
       "for SOLAR 10.7B-Instruct in Appendix C .\n",
       "\n",
       "  \n",
       "4.3 Ablation Studies\n",
       "\n",
       "  \n",
       "We present ablation studies for both the instruction  \n",
       "and alignment tuning stages. Note that the evalua-  \n",
       "tion results for the following studies are ran locally  \n",
       "and may vary from results obtained by submitting  \n",
       "to the Open LLM Leaderboard.\n",
       "\n",
       "  \n",
       "4.3.1 Instruction Tuning\n",
       "\n",
       "  \n",
       "Ablation on the training datasets. We present  \n",
       "ablation studies using different training datasets  \n",
       "for the instruction tuning in Tab. 3 . The ablated  \n",
       "models are preﬁxed with SFT for supervised ﬁne-  \n",
       "tuning. ‘SFT v1’ only uses the Alpaca-GPT4  \n",
       "dataset, whereas ‘SFT v2’ also uses the OpenOrca  \n",
       "dataset. ‘SFT v3’ uses the Synth. Math-Instruct  \n",
       "dataset along with the datasets us"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from markdownify import markdownify as md\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "md_text = md(docs[0].page_content)\n",
    "display(Markdown(md_text[:5000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "\n",
    "llm = ChatUpstage()\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please provide most correct answer from the following context. \n",
    "    If the answer is not present in the context, please write \"The information is not present in the context.\"\n",
    "    ---\n",
    "    Question: {question}\n",
    "    ---\n",
    "    Context: {Context}\n",
    "    \"\"\"\n",
    ")\n",
    "chain = prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Table 2 presents the evaluation results for the Open LLM Leaderboard, which includes SOLAR 10.7B and SOLAR 10.7B-Instruct along with other top-performing models. The table reports the scores for six tasks mentioned in Sec. 4.1, including H6 (average of six tasks), as well as the size of the models in units of billions of parameters. The type indicates the training stage of the model and is chosen from {Pretrained, Instruction-tuned, Alignment-tuned}. Models based on SOLAR 10.7B are colored purple, and the best scores for H6 and the individual tasks are shown in bold.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"Explain Table 2?\", \"Context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The MMLU scores of SOLAR 10.7B is 65.48.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is MMLU scores of SOLAR 10.7B?\", \"Context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The ARC of Falcon 180B is 69.45.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is ARC of Falcon 180B?\", \"Context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mistral 7B-Instruct-v0.2 has an MMLU score of 60.78.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is MMLU scores of Mistral?\", \"Context\": md_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise \n",
    "Sometimes, even if we provide a table in Markdown or HTML format, the Large Language Model (LLM) may not extract the information correctly. How can you fix this issue?\n",
    "\n",
    "Hint: Consider using CoT, a few-shot learning approach or a divide and conquer strategy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
